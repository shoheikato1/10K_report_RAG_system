{"cells":[{"cell_type":"markdown","id":"5bb1b56b-63b2-4438-b250-7dd55e5f6ac3","metadata":{"id":"5bb1b56b-63b2-4438-b250-7dd55e5f6ac3"},"source":["### Step 0: Install dependencies"]},{"cell_type":"code","execution_count":null,"id":"45783581-574c-4687-b8f5-e3d972ea8594","metadata":{"id":"45783581-574c-4687-b8f5-e3d972ea8594"},"outputs":[],"source":["#!pip install ipywidgets PyPDF2 faiss-cpu sentence-transformers nltk jupyter ipython\n","#!pip install openai"]},{"cell_type":"markdown","id":"5cc3933e-83e2-4b06-aa9b-2fb70e1e5a4d","metadata":{"id":"5cc3933e-83e2-4b06-aa9b-2fb70e1e5a4d"},"source":["### Step 1: Import dependencies"]},{"cell_type":"code","execution_count":null,"id":"024bbe70-42d6-458d-9dd2-f66729234352","metadata":{"id":"024bbe70-42d6-458d-9dd2-f66729234352"},"outputs":[],"source":["import os\n","import io\n","import gc\n","import faiss\n","import numpy as np\n","from PyPDF2 import PdfReader\n","from sentence_transformers import SentenceTransformer\n","import torch\n","import time\n","from tqdm.auto import tqdm\n","from tkinter import Tk, filedialog\n","import tkinter as tk\n","from typing import List, Dict, Any, Tuple\n","from openai import OpenAI\n","import re\n","from collections import defaultdict"]},{"cell_type":"markdown","id":"99690f32-e54f-446c-8630-62e205edb46e","metadata":{"id":"99690f32-e54f-446c-8630-62e205edb46e"},"source":["### Step 2: Develop RAG System"]},{"cell_type":"code","execution_count":null,"id":"838715b4-e097-4ea1-84a9-5fa19e7b648b","metadata":{"id":"838715b4-e097-4ea1-84a9-5fa19e7b648b"},"outputs":[],"source":["class RAGFinancialAnalyzer:\n","    #\n","    def __init__(self, openai_api_key):\n","        self.chunks = []\n","        self.index = None\n","        self.model = None\n","        self.client = OpenAI(api_key=openai_api_key)\n","        self.company_mappings = {\n","            'TRV': 'Travelers',\n","            'ALL': 'Allstate',\n","            'CB': 'Chubb',\n","            'PRG': 'Progressive'\n","        }\n","        print(\"RAG Financial Analyzer initialized successfully.\")\n","# ---------------------------------------------------------------------------------\n","# Chunking & Preprocessing\n","# ---------------------------------------------------------------------------------\n","    def _extract_company_code(self, filename: str) -> str:\n","        \"\"\"Extract company code from filename\"\"\"\n","        for code in self.company_mappings:\n","            if code in filename:\n","                return code\n","        return \"Unknown\"\n","\n","    def _clean_text(self, text: str) -> str:\n","        \"\"\"Clean and standardize text content\"\"\"\n","        text = re.sub(r'\\s+', ' ', text)\n","        text = re.sub(r'[^\\x20-\\x7E]', '', text)\n","        return text.strip()\n","\n","    def _get_company_name(self, company_code: str) -> str:\n","        \"\"\"Get full company name from code\"\"\"\n","        return self.company_mappings.get(company_code, company_code)\n","\n","    def _organize_by_company(self, chunks: List[Dict]) -> Dict[str, List[str]]:\n","        \"\"\"Organize text chunks by company\"\"\"\n","        company_texts = defaultdict(list)\n","        for chunk in chunks:\n","            company_code = self._extract_company_code(chunk[\"metadata\"][\"source\"])\n","            company_name = self._get_company_name(company_code)\n","            text_with_metadata = f\"{chunk['text']} [Source: {chunk['metadata']['source']}, Page: {chunk['metadata']['page']}]\"\n","            company_texts[company_name].append(text_with_metadata)\n","        return company_texts\n","\n","    def _create_structured_context(self, chunks: List[Dict], max_tokens: int = 4000) -> Tuple[str, List[str]]:\n","        \"\"\"Create structured context organized by company\"\"\"\n","        company_texts = self._organize_by_company(chunks)\n","        context = \"\"\n","        sources = []\n","        current_length = 0\n","\n","        for company, texts in company_texts.items():\n","            company_section = f\"\\n{company} Analysis:\\n\"\n","            for text in texts:\n","                # Extract source information\n","                source_match = re.search(r'\\[Source: (.*?), Page: (\\d+)\\]', text)\n","                if source_match:\n","                    source = f\"{source_match.group(1)} (page {source_match.group(2)})\"\n","                    if source not in sources:\n","                        sources.append(source)\n","\n","                # Remove source information from text for context\n","                text_without_source = re.sub(r'\\[Source:.*?\\]', '', text)\n","\n","                chunk_tokens = len(text_without_source) / 4\n","                if current_length + chunk_tokens > max_tokens:\n","                    break\n","\n","                company_section += f\"- {text_without_source}\\n\"\n","                current_length += chunk_tokens\n","\n","            context += company_section\n","\n","        return context.strip(), sources\n","# ---------------------------------------------------------------------------------\n","# Embedding & Indexing\n","# ---------------------------------------------------------------------------------\n","\n","    def upload_and_process(self):\n","        \"\"\"Process uploaded financial documents\"\"\"\n","        try:\n","            print(\"Select your financial statement PDFs...\")\n","            root = tk.Tk()\n","            root.withdraw()\n","            file_paths = filedialog.askopenfilenames(\n","                title=\"Select Financial Statement PDFs\",\n","                filetypes=[(\"PDF files\", \"*.pdf\")]\n","            )\n","            root.destroy()\n","\n","            if not file_paths:\n","                print(\"No files were selected.\")\n","                return\n","\n","            print(f\"\\nProcessing {len(file_paths)} documents...\")\n","            start_time = time.time()\n","\n","            for file_path in file_paths:\n","                try:\n","                    filename = os.path.basename(file_path)\n","                    company_code = self._extract_company_code(filename)\n","                    company_name = self._get_company_name(company_code)\n","                    print(f\"\\nProcessing {company_name} ({filename})...\")\n","\n","                    with open(file_path, 'rb') as file:\n","                        pdf = PdfReader(file)\n","\n","                        for page_num, page in enumerate(pdf.pages):\n","                            text = page.extract_text()\n","                            if not text:\n","                                continue\n","\n","                            clean_text = self._clean_text(text)\n","                            paragraphs = clean_text.split('\\n\\n')\n","\n","                            for para in paragraphs:\n","                                if len(para.strip()) > 50:\n","                                    self.chunks.append({\n","                                        \"text\": para.strip(),\n","                                        \"metadata\": {\n","                                            \"source\": filename,\n","                                            \"page\": page_num + 1,\n","                                            \"company\": company_name\n","                                        }\n","                                    })\n","\n","                except Exception as e:\n","                    print(f\"Error processing {filename}: {str(e)}\")\n","                    continue\n","\n","            if not self.chunks:\n","                print(\"No text could be extracted from the documents.\")\n","                return\n","\n","            print(f\"\\nGenerating embeddings for {len(self.chunks)} text segments...\")\n","            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n","            texts = [chunk[\"text\"] for chunk in self.chunks]\n","            embeddings = self.model.encode(\n","                texts,\n","                batch_size=32,\n","                show_progress_bar=True\n","            )\n","\n","            print(\"\\nBuilding search index...\")\n","            dimension = embeddings.shape[1]\n","            self.index = faiss.IndexFlatL2(dimension)\n","            self.index.add(embeddings.astype('float32'))\n","\n","            del embeddings\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","            gc.collect()\n","\n","            processing_time = time.time() - start_time\n","            print(f\"\\nProcessing complete! Time taken: {processing_time:.2f} seconds\")\n","\n","        except Exception as e:\n","            print(f\"An error occurred during processing: {str(e)}\")\n","\n","    def analyze(self, question: str, top_k: int = 10):\n","        \"\"\"Generate analytical responses with company-specific insights\"\"\"\n","        if not self.index or not self.chunks:\n","            print(\"Please process documents first using upload_and_process()\")\n","            return\n","\n","        try:\n","            print(f\"Analyzing: {question}\\n\")\n","\n","# ---------------------------------------------------------------------------------\n","# Retrieval Process\n","# ---------------------------------------------------------------------------------\n","\n","            query_embedding = self.model.encode([question])\n","            D, I = self.index.search(query_embedding.astype('float32'), top_k)\n","\n","            selected_chunks = [self.chunks[idx] for idx in I[0]]\n","# -----------------------------------------------------------------------------------\n","# Augmentation Process\n","# -----------------------------------------------------------------------------------\n","\n","            context, sources = self._create_structured_context(selected_chunks)\n","\n","            prompt = f\"\"\"As an expert financial analyst, provide a detailed analysis of the following financial statements,\n","organized by company. Focus on specific findings for each company mentioned in the documents.\n","\n","Question: {question}\n","\n","{context}\n","\n","Please provide a comprehensive analysis that:\n","1. Clearly separates findings by company\n","2. Includes specific numerical evidence\n","3. Cites sources (document and page number) for each major point\n","4. Explains business implications for each company\n","\n","Analysis:\"\"\"\n","\n","# -----------------------------------------------------------------------------------\n","# Generation Process\n","# -----------------------------------------------------------------------------------\n","\n","            response = self.client.chat.completions.create(\n","                model=\"gpt-4\",\n","                messages=[\n","                    {\"role\": \"system\", \"content\": \"\"\"You are an expert financial analyst.\n","                    Organize your analysis by company, clearly stating which company you're discussing for each point.\n","                    Always cite specific sources and page numbers for your findings.\n","                    Focus on providing company-specific insights and comparisons where relevant.\"\"\"},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                temperature=0.7,\n","                max_tokens=1500\n","            )\n","\n","            print(\"Analysis:\")\n","            print(\"-\" * 80)\n","            print(response.choices[0].message.content)\n","            print(\"\\nSources Referenced:\")\n","            for source in sources:\n","                print(f\"- {source}\")\n","            print(\"-\" * 80)\n","\n","        except Exception as e:\n","            print(f\"Error during analysis: {str(e)}\")"]},{"cell_type":"markdown","id":"9c011eae-967b-4454-97eb-bd4348cb76a7","metadata":{"id":"9c011eae-967b-4454-97eb-bd4348cb76a7"},"source":["### Step 3: Insert Open API Key"]},{"cell_type":"code","execution_count":null,"id":"8139c70e-09ff-4c84-93ca-21838ef16231","metadata":{"id":"8139c70e-09ff-4c84-93ca-21838ef16231","outputId":"0e65d775-1d7b-4f4b-b03f-f1d2f61c1ded"},"outputs":[{"name":"stdin","output_type":"stream","text":["Enter your OpenAI API key:  ········\n"]},{"name":"stdout","output_type":"stream","text":["RAG Financial Analyzer initialized successfully.\n","RAG system initialized successfully!\n"]}],"source":["from getpass import getpass\n","\n","# Securely prompt user for the API key (input will be hidden)\n","openai_api_key = getpass('Enter your OpenAI API key: ')\n","\n","# Initialize both components with the provided key\n","analyzer = RAGFinancialAnalyzer(openai_api_key)\n","\n","print(\"RAG system initialized successfully!\")"]},{"cell_type":"markdown","id":"aeae0fda-831a-415f-b24c-0ad826ad3249","metadata":{"id":"aeae0fda-831a-415f-b24c-0ad826ad3249"},"source":["### Step 4: Upload Financial Reports"]},{"cell_type":"code","execution_count":null,"id":"1a0942e3-49ae-4452-8729-0a0e66062a15","metadata":{"id":"1a0942e3-49ae-4452-8729-0a0e66062a15","outputId":"11c3296e-7661-4957-9d1c-d4363c3bc329","colab":{"referenced_widgets":["7c8de42b989e4543a91ba73e91263fe9"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["Select your financial statement PDFs...\n","\n","Processing 4 documents...\n","\n","Processing Allstate (ALL_10K_FY2023.pdf)...\n","\n","Processing Chubb (CB_10K_FY2023.pdf)...\n","\n","Processing Progressive (PRG_10K_FY2023.pdf)...\n","\n","Processing Travelers (TRV_10K_FY2023.pdf)...\n","\n","Generating embeddings for 1020 text segments...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c8de42b989e4543a91ba73e91263fe9","version_major":2,"version_minor":0},"text/plain":["Batches:   0%|          | 0/32 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\n","Building search index...\n","\n","Processing complete! Time taken: 70.28 seconds\n"]}],"source":["analyzer.upload_and_process()"]},{"cell_type":"markdown","id":"66107bc3-71b4-4b6b-ae2b-406079519404","metadata":{"id":"66107bc3-71b4-4b6b-ae2b-406079519404"},"source":["### Step 5: Ask Questions"]},{"cell_type":"code","execution_count":null,"id":"21c88496-3826-4526-931e-857f0efbd69e","metadata":{"id":"21c88496-3826-4526-931e-857f0efbd69e","outputId":"08c6a5d8-1258-4e3f-d158-5d967528602a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Analyzing: What are the causes driving the largest amount of losses across all companies? Please cite all the references from the source. Make sure I know which company is talked about.\n","\n","Analysis:\n","--------------------------------------------------------------------------------\n","Allstate Corporation:\n","\n","1. Allstate's net loss applicable to common shareholders was $316 million in 2023, which was an improvement from the $1.39 billion net loss in 2022. The company attributes this improvement to better underwriting results and net gains on equity valuations (2023 Form 10-K Consolidated net income, p.38).\n","2. The company's total revenue increased by 11.1% to $57.09 billion in 2023, primarily due to a 10.4% increase in property and casualty insurance premiums. The company also had net gains on equity valuations in 2023 as compared to losses in 2022 (2023 Form 10-K Total revenue, p.38).\n","3. Allstate's major risks include insurance and financial services, business strategy and operations, and macro, regulatory and risk environment. Specific risks include claim frequency and severity volatility, catastrophes and severe weather, investment results being subject to market volatility and valuation judgments, etc. (2023 Form 10-K Part I - Item 1A. Risk Factors and Other Disclosures Item 1A. Risk Factors Summary, p.22).\n","   \n","Business implications: Allstate's improved financial performance in 2023 indicates a positive trend, but the company needs to continue managing its risk factors effectively to maintain this trajectory.\n","\n","Travelers Companies:\n","\n","1. Travelers reported a net income of $2.99 billion in 2023, with net earned premiums of $37.76 billion. The company also had catastrophe losses of $2.99 billion ($2.36 billion after-tax) and a combined ratio of 97.0% (Item 7. MANAGEMENTS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS, p.61).\n","2. Travelers had net pre-tax realized investment losses of $105 million in 2023, primarily due to movements in global financial markets and interest rates (Item 7. MANAGEMENTS DISCUSSION AND ANALYSIS OF FINANCIAL CONDITION AND RESULTS OF OPERATIONS, p.91).\n","   \n","Business implications: While Travelers has a robust income and premium earnings, the company needs to manage its investment portfolio carefully to minimize losses. Also, a combined ratio of 97.0% indicates underwriting profitability, but the company must manage catastrophe losses effectively.\n","\n","Chubb Limited:\n","\n","Unfortunately, the documents did not provide any specific financial information about Chubb Limited. Please provide relevant financial statements or reports for a comprehensive analysis of Chubb Limited.\n","\n","Sources Referenced:\n","- ALL_10K_FY2023.pdf (page 24)\n","- ALL_10K_FY2023.pdf (page 40)\n","- TRV_10K_FY2023.pdf (page 61)\n","- TRV_10K_FY2023.pdf (page 91)\n","- TRV_10K_FY2023.pdf (page 73)\n","- CB_10K_FY2023.pdf (page 63)\n","--------------------------------------------------------------------------------\n"]}],"source":["analyzer.analyze(\"What are the causes driving the largest amount of losses across all companies? Please cite all the references from the source. Make sure I know which company is talked about.\")"]},{"cell_type":"code","execution_count":null,"id":"e08976e4-8485-49ee-9fcb-11e6e5048cb9","metadata":{"id":"e08976e4-8485-49ee-9fcb-11e6e5048cb9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}